{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.nvib_sa_transformer_encoder import (\n",
    "    NVIBTransformerEncoder,\n",
    "    NVIBTransformerEncoderLayer,\n",
    ")\n",
    "from models.seq2seq_lightning import Seq2SeqLightning\n",
    "from models.transformer import *\n",
    "from models.transformer_encoder import (\n",
    "    CustomTransformerEncoder,\n",
    "    CustomTransformerEncoderLayer,\n",
    ")\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NVIBSaTransformerLightning(Seq2SeqLightning):\n",
    "    def __init__(self, args, **kwargs):\n",
    "        super().__init__(args, **kwargs)\n",
    "\n",
    "        # Tokenizer\n",
    "        self.tokenizer = CharizardTokenizer(model_max_length=args.max_length)\n",
    "        # Model\n",
    "        self.model = NVIBSaTransformer(tokenizer=self.tokenizer, **vars(args))\n",
    "\n",
    "        # Nvib\n",
    "        self.lambda_klg = args.klg_lambda\n",
    "        self.lambda_kld = args.kld_lambda\n",
    "\n",
    "        # Logging metrics\n",
    "        self.log_bleu = True\n",
    "        self.log_chrf = True\n",
    "        self.plot_encoder_attention = True\n",
    "        self.plot_cross_attention = True\n",
    "        self.model_type = \"NVIBSaTransformer\"\n",
    "        self.is_nvib = True\n",
    "        self.weighted_kl = args.weighted_kl\n",
    "\n",
    "        # Initialization\n",
    "        init_weights(self.model)\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "class NVIBSaTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A vanilla Transformer Encoder-Decoder in Pytorch\n",
    "\n",
    "    Data format:\n",
    "    SRC: ... [EOS]\n",
    "    TGT: ... [EOS]\n",
    "    Encoder_input(SRC): ... [EOS]\n",
    "    Decoder_input(TGT): [SOS] ...\n",
    "\n",
    "    For an autoencoder x -> x (SRC = TGT)\n",
    "        The loss function requires SRC and logits.\n",
    "    For different models x -> y (Eg: translation SRC != TGT)\n",
    "        The loss function requires TGT and logits.\n",
    "\n",
    "    If we keep this format the attention masks for padding are identical for autoencoder's encoder + decoder .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer=tokenizer, **kwargs)\n",
    "        print(kwargs)\n",
    "        self.d_model = kwargs[\"d_model\"]\n",
    "        self.nhead = kwargs[\"nhead\"]\n",
    "        self.dim_feedforward = kwargs[\"dim_feedforward\"]\n",
    "        self.dropout = kwargs[\"dropout\"]\n",
    "        self.num_encoder_layers = kwargs[\"num_encoder_layers\"]\n",
    "        self.num_nvib_encoder_layers = kwargs[\"num_nvib_encoder_layers\"]\n",
    "        self.num_decoder_layers = kwargs[\"num_decoder_layers\"]\n",
    "        self.kappa = kwargs[\"kappa\"]\n",
    "        self.delta = kwargs[\"delta\"]\n",
    "\n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = CustomTransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nhead,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "\n",
    "        # NVIB Transformer encoder layer\n",
    "        nvib_transformer_encoder_layer = NVIBTransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nhead,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=\"relu\",\n",
    "            kappa=self.kappa,\n",
    "            delta=self.delta,\n",
    "        )\n",
    "        encoder_norm = nn.LayerNorm(self.d_model, eps=1e-5)\n",
    "        self.encoder = CustomTransformerEncoder(\n",
    "            encoder_layer, self.num_encoder_layers, encoder_norm\n",
    "        )\n",
    "        self.nvib_transformer_encoder = NVIBTransformerEncoder(\n",
    "            nvib_transformer_encoder_layer, self.num_nvib_encoder_layers, encoder_norm\n",
    "        )\n",
    "\n",
    "        # Transformer decoder\n",
    "        decoder_layer = CustomTransformerDecoderLayer(\n",
    "            self.d_model,\n",
    "            self.nhead,\n",
    "            self.dim_feedforward,\n",
    "            self.dropout,\n",
    "        )\n",
    "        decoder_norm = nn.LayerNorm(self.d_model, eps=1e-5)\n",
    "        self.decoder = CustomTransformerDecoder(\n",
    "            decoder_layer, self.num_decoder_layers, decoder_norm\n",
    "        )\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.decoder_start_token_id = tokenizer.cls_token_id\n",
    "        self.args = kwargs\n",
    "        self.embedding = nn.Embedding(tokenizer.vocab_size, self.d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model)\n",
    "        self.output_proj = nn.Linear(self.d_model, tokenizer.vocab_size)\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "\n",
    "    def encode(self, src, src_key_padding_mask):\n",
    "        \"\"\"\n",
    "        Encode the input ids to embeddings and then pass to the transformer encoder\n",
    "        :param src: source token ids [Ns, B]\n",
    "        :param src_key_padding_mask: Trues where to mask [B,Ns]\n",
    "        :return: memory: [Ns,B,H]\n",
    "        \"\"\"\n",
    "        # Add position encodings + Embeddings\n",
    "        src = self.positional_encoding(self.drop(self.embedding(src)))  # [Ns,B,H]\n",
    "\n",
    "        # Transformer encoder\n",
    "        memory1, attention1 = self.encoder(\n",
    "            src, src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [Ns,B,H]\n",
    "\n",
    "        # NVIB Transformer encoder\n",
    "        memory2, attention2, klg, kld, latent_dict = self.nvib_transformer_encoder(\n",
    "            memory1[-1], src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [Ns,B,H]\n",
    "        # Concatenate the attention lists\n",
    "        attention = attention1 + attention2\n",
    "        return memory2, attention, klg, kld, latent_dict, memory1\n",
    "\n",
    "    def decode(\n",
    "        self, tgt, z, memory_key_padding_mask, tgt_key_padding_mask, *args, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tgt: target token ids [Nt,B]\n",
    "        :param z: output from the latent layer [Nl,B,H]\n",
    "        :param memory_key_padding_mask: mask for latent layer [B, Nl] (typically Ns = Nl)\n",
    "        :param tgt_key_padding_mask: target mask [B,Nt]\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return: logits over the vocabulary [Nt,B,V]\n",
    "        \"\"\"\n",
    "\n",
    "        # Add position encodings + Embeddings\n",
    "        tgt = self.positional_encoding(self.drop(self.embedding(tgt)))  # [Nt,B,H]\n",
    "        # Generate target teacher forcing mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(\n",
    "            tgt.device\n",
    "        )  # [Nt, Nt]\n",
    "        output, attention = self.decoder(\n",
    "            tgt=tgt,  # [Nt,B,H]\n",
    "            memory=z,  # [Nt,B,H]\n",
    "            tgt_mask=tgt_mask,  # [Nt,Nt]\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,  # [B,Nt]\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "            # latent_dict=kwargs[\"latent_dict\"] if \"latent_dict\" in kwargs else None,\n",
    "        )  # [B,Nl]\n",
    "        logits = self.output_proj(output)  # [Nt,B,V]\n",
    "        return logits, attention\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate autoregressively without teacher forcing\n",
    "        :param z: output from the latent layer [Nl,B,H]\n",
    "        :param memory_key_padding_mask: mask from the latent layer [B,Nl]\n",
    "        :param max_len: maximum generation length\n",
    "        :param tokenizer: tokenizer\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return: logits [Nt,B,V] and list of predictions\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        src_key_padding_mask = ~(input_ids.bool()).transpose(0, 1)  # [B,Ns]\n",
    "        memory, _, _, _, self_attention_latent, _ = self.encode(\n",
    "            input_ids, src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [Ns,B,H]\n",
    "\n",
    "        # Mask the src_key_padding_mask with the final latent layer's pi for cross attention\n",
    "        src_key_padding_mask = src_key_padding_mask + self_attention_latent[-1][\n",
    "            \"alpha\"\n",
    "        ].squeeze(-1).transpose(0, 1)[:, 1:].le(0.1)\n",
    "\n",
    "        # Soft weighting of vectors\n",
    "        # memory = memory * self_attention_latent[-1][\"pi\"][1:, :, :]\n",
    "\n",
    "        # latent layer\n",
    "        latent_output_dict = self.latent_layer(memory, src_key_padding_mask)\n",
    "        memory_key_padding_mask = latent_output_dict[\"memory_key_padding_mask\"]\n",
    "        z = latent_output_dict[\"z\"]\n",
    "\n",
    "        # Initialise target ids with BOS token\n",
    "        target_ids = (\n",
    "            torch.tensor([[self.decoder_start_token_id]])\n",
    "            .expand(memory_key_padding_mask.shape[0], -1)\n",
    "            .T.to(memory_key_padding_mask.device)\n",
    "        )  # [1, B]\n",
    "        # For each token in length\n",
    "        for token_idx in range(max_new_tokens):\n",
    "            # Decode the target ids regressively\n",
    "            logits, _ = self.decode(\n",
    "                target_ids,\n",
    "                z,\n",
    "                memory_key_padding_mask,\n",
    "                None,\n",
    "                # latent_dict=self_attention_latent[-1]\n",
    "            )  # [token_idx, B, V]\n",
    "            # Select only the final set of logits\n",
    "            prediction = logits[-1, :, :].unsqueeze(0)  # [target_ids1,B,V]\n",
    "            # Get prediction over vocabulary and return index\n",
    "            prediction = prediction.argmax(-1)  # [1,B]\n",
    "            # Concatenate the predictions to form next token_ids\n",
    "            target_ids = torch.cat((target_ids, prediction), dim=0)  # [token_index, B]\n",
    "        \n",
    "\n",
    "        # Decode into a sentence\n",
    "        # predictions = [tokenizer.decode(encoded) for encoded in target_ids[1:, :].T]  # list [B]\n",
    "        return target_ids[1:, :]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        decoder_input_ids,\n",
    "        labels,\n",
    "        attention_mask,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass for all transformer models\n",
    "\n",
    "        :param src: the sequence to the encoder (required). [Ns,B]\n",
    "        :param tgt: the sequence  nce to the decoder (required). [Nt,B]\n",
    "        :param src_mask: the additive mask for the src sequence (optional). [Ns, Ns]\n",
    "        :param tgt_mask: the additive mask for the tgt sequence (optional). [Nt, Nt]\n",
    "        :param memory_mask: the additive mask for the encoder output (optional). [Nt,Ns]\n",
    "        :param src_key_padding_mask: the ByteTensor mask for src keys per batch (optional). [B,Ns]\n",
    "        :param tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional). [B,Nt]\n",
    "        :param memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).[B,Nl]\n",
    "        :return: logits and latent dimension dictionary\n",
    "\n",
    "        Check out here for more info masks on https://stackoverflow.com/questions/62170439/difference-between-src-mask-and-src-key-padding-mask\n",
    "        The memory ones are interesting. I use memory_key_padding_mask to mask the tokens in the latent space.\n",
    "\n",
    "        \"\"\"\n",
    "        # Reformat the attention mask\n",
    "        src_key_padding_mask = ~(attention_mask.bool())\n",
    "        tgt_key_padding_mask = decoder_input_ids.transpose(0, 1) == self.pad_token_id\n",
    "\n",
    "        # Encode\n",
    "        (\n",
    "            memory,\n",
    "            encoder_attention,\n",
    "            klg,\n",
    "            kld,\n",
    "            self_attention_latent,\n",
    "            old_memory,\n",
    "        ) = self.encode(\n",
    "            input_ids, src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [Ns,B,H]\n",
    "\n",
    "        # Mask the src_key_padding_mask with the final latent layer's pi for cross attention\n",
    "        alpha = self_attention_latent[-1][\"alpha\"].squeeze(-1).transpose(0, 1)  # [B, Ns]\n",
    "        if alpha.size(1) != src_key_padding_mask.size(1):\n",
    "            # If alpha has an extra token (prior token), remove it\n",
    "            alpha = alpha[:, 1:]\n",
    "            # If alpha is still smaller, pad it to match src_key_padding_mask\n",
    "            if alpha.size(1) < src_key_padding_mask.size(1):\n",
    "                alpha = F.pad(alpha, (0, src_key_padding_mask.size(1) - alpha.size(1)), value=0)\n",
    "        src_key_padding_mask = src_key_padding_mask + alpha.le(0.1)\n",
    "\n",
    "        # Soft weighting of vectors\n",
    "        # memory = memory * self_attention_latent[-1][\"pi\"][1:, :, :]\n",
    "\n",
    "        # latent layer\n",
    "        latent_output_dict = self.latent_layer(memory, src_key_padding_mask)\n",
    "        # Decode\n",
    "        output, decoder_attention = self.decode(\n",
    "            tgt=decoder_input_ids,  # [Nt,B,H]\n",
    "            z=latent_output_dict[\"z\"],  # [Nl,B,H]\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,  # [B,Nt]\n",
    "            memory_key_padding_mask=latent_output_dict[\"memory_key_padding_mask\"],\n",
    "            # latent_dict=self_attention_latent[-1],\n",
    "        )  # [B,Nl]\n",
    "\n",
    "        return {\n",
    "            \"logits\": output,  # [Nt, B, V]\n",
    "            \"encoder_attentions\": encoder_attention,  # Self attention\n",
    "            \"cross_attentions\": decoder_attention,  # Cross attention\n",
    "            \"kl_gaussian\": klg,\n",
    "            \"kl_dirichlet\": kld,\n",
    "            \"latent_dict_list\": self_attention_latent,\n",
    "            \"old_memory\": old_memory,\n",
    "            \"old_memory_mask\": ~(attention_mask.bool()),\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # Paths + Naming\n",
    "    experiment_name: str = \"nvib-paper\"\n",
    "    project_name: str = \"nvib_selfattention\"\n",
    "    output_dir: str = \"outputs\"\n",
    "    entity: str = None\n",
    "\n",
    "    # Data\n",
    "    data: str = \"wikitext\"\n",
    "    data_subset: str = \"wikitext-2-raw-v1\"\n",
    "    num_workers: int = 17\n",
    "    max_length: int = 128\n",
    "\n",
    "    # Model\n",
    "    model: str = \"NVIBSaTransformer\"\n",
    "\n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    fast_dev_run: bool = False\n",
    "    fp16: bool = False\n",
    "\n",
    "    # Transformer\n",
    "    d_model: int = 512\n",
    "    d_compress_model: int = 512\n",
    "    nhead: int = 8\n",
    "    num_encoder_layers: int = 3\n",
    "    num_decoder_layers: int = 2\n",
    "    dim_feedforward: int = 512\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Token deletion probability\n",
    "    deletion_prob: float = 0\n",
    "    deletion_type: str = \"token\"  # choices: [\"token\", \"word\", \"token_word\", \"None\"]\n",
    "\n",
    "    # NVIB\n",
    "    num_nvib_encoder_layers: int = 1\n",
    "    kappa: float = 1\n",
    "    delta: float = 1\n",
    "    klg_lambda: float = 0.001\n",
    "    kld_lambda: float = 1\n",
    "    kl_annealing_type: str = \"constant\"\n",
    "    weighted_kl: bool = True\n",
    "\n",
    "    # Learning rate + Schedulers\n",
    "    learning_rate: float = 1e-4\n",
    "    lr_scheduler: bool = False\n",
    "    perc_warmup: float = 0\n",
    "    batch_size: int = 1\n",
    "\n",
    "    # PL Trainer\n",
    "    max_time: str = None\n",
    "    max_steps: int = 1000\n",
    "    max_epochs: int = None\n",
    "    accumulate_grad_batches: int = 1\n",
    "    checkpoint_interval: int = 100\n",
    "    validation_interval: int = None\n",
    "\n",
    "    def __init__(self):\n",
    "        # Make sure all attributes are properly initialized\n",
    "        for attr_name in dir(self):\n",
    "            if not attr_name.startswith('__'):\n",
    "                setattr(self, attr_name, getattr(self, attr_name))\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accumulate_grad_batches': 1, 'batch_size': 1, 'checkpoint_interval': 100, 'd_compress_model': 512, 'd_model': 512, 'data': 'wikitext', 'data_subset': 'wikitext-2-raw-v1', 'deletion_prob': 0, 'deletion_type': 'token', 'delta': 1, 'dim_feedforward': 512, 'dropout': 0.1, 'entity': None, 'experiment_name': 'nvib-paper', 'fast_dev_run': False, 'fp16': False, 'kappa': 1, 'kl_annealing_type': 'constant', 'kld_lambda': 1, 'klg_lambda': 0.001, 'learning_rate': 0.0001, 'lr_scheduler': False, 'max_epochs': None, 'max_length': 128, 'max_steps': 1000, 'max_time': None, 'model': 'NVIBSaTransformer', 'nhead': 8, 'num_decoder_layers': 2, 'num_encoder_layers': 3, 'num_nvib_encoder_layers': 1, 'num_workers': 17, 'output_dir': 'outputs', 'perc_warmup': 0, 'project_name': 'nvib_selfattention', 'seed': 42, 'validation_interval': None, 'weighted_kl': True}\n"
     ]
    }
   ],
   "source": [
    "model = {\n",
    "    \"Transformer\": TransformerLightning,\n",
    "    \"NVIBSaTransformer\": NVIBSaTransformerLightning,\n",
    "}[args.model](args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model type: NVIBSaTransformer\n",
      "d_model: 512\n",
      "num_nvib_encoder_layers: 1\n",
      "weighted_kl: True\n",
      "klg_lambda: 0.001\n",
      "kld_lambda: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model type: {args.model}\")\n",
    "print(f\"d_model: {args.d_model}\")\n",
    "print(f\"num_nvib_encoder_layers: {args.num_nvib_encoder_layers}\")\n",
    "print(f\"weighted_kl: {args.weighted_kl}\")\n",
    "print(f\"klg_lambda: {args.klg_lambda}\")\n",
    "print(f\"kld_lambda: {args.kld_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/project/phan/tqn/new_Adapter/nvib_selfattention/outputs/nvib_selfattention/nvib-debug4/\"\n",
    "CHECKPOINT_PATH = get_checkpoint_path(OUTPUT_PATH)\n",
    "BEST_MODEL_PATH = \"/project/phan/tqn/new_Adapter/nvib_selfattention/outputs/nvib_selfattention/nvib-debug4/best_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/phan/tqn/new_Adapter/nvib_selfattention/outputs/nvib_selfattention/nvib-debug4/epoch=40-step=6500.ckpt\n"
     ]
    }
   ],
   "source": [
    "print(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model\n",
      "Loading model:  /project/phan/tqn/new_Adapter/nvib_selfattention/outputs/nvib_selfattention/nvib-debug4/best_model.ckpt\n",
      "{'accumulate_grad_batches': 1, 'batch_size': 1, 'checkpoint_interval': 100, 'd_compress_model': 512, 'd_model': 512, 'data': 'wikitext', 'data_subset': 'wikitext-2-raw-v1', 'deletion_prob': 0, 'deletion_type': 'token', 'delta': 1, 'dim_feedforward': 512, 'dropout': 0.1, 'entity': None, 'experiment_name': 'nvib-paper', 'fast_dev_run': False, 'fp16': False, 'kappa': 1, 'kl_annealing_type': 'constant', 'kld_lambda': 1, 'klg_lambda': 0.001, 'learning_rate': 0.0001, 'lr_scheduler': False, 'max_epochs': None, 'max_length': 128, 'max_steps': 1000, 'max_time': None, 'model': 'NVIBSaTransformer', 'nhead': 8, 'num_decoder_layers': 2, 'num_encoder_layers': 3, 'num_nvib_encoder_layers': 1, 'num_workers': 17, 'output_dir': 'outputs', 'perc_warmup': 0, 'project_name': 'nvib_selfattention', 'seed': 42, 'validation_interval': None, 'weighted_kl': True}\n",
      "Loading W&B ID\n"
     ]
    }
   ],
   "source": [
    "model, wandb_id = create_or_load_model(OUTPUT_PATH, BEST_MODEL_PATH, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIBSaTransformerLightning(\n",
      "  (model): NVIBSaTransformer(\n",
      "    (encoder): CustomTransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-2): 3 x CustomTransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): CustomTransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x CustomTransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (embedding): Embedding(107, 512, padding_idx=0)\n",
      "    (positional_encoding): PositionalEncoding()\n",
      "    (output_proj): Linear(in_features=512, out_features=107, bias=True)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (nvib_transformer_encoder): NVIBTransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0): NVIBTransformerEncoderLayer(\n",
      "          (nvib_layer): Nvib(\n",
      "            (alpha_activation): Exponential()\n",
      "            (mu_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (logvar_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (alpha_proj): Quadratic(\n",
      "              (linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "              (quadratic): Linear(in_features=512, out_features=1, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (self_attn): DenoisingMultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  0,  50,  21,  28,  28,  31,  80, 101,  24,  31,  39, 101,  17,  34,\n",
      "          21, 101,  41,  31,  37,  89,   1]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_text = \"Hello, how are you?\"\n",
    "inputs = model.tokenizer(input_text, \n",
    "                      padding=True, \n",
    "                      truncation=True, \n",
    "                      return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = inputs[\"input_ids\"].transpose(0, 1).to('cuda')  # [Ns,B]\n",
    "attention_mask = inputs[\"attention_mask\"].to('cuda')  # [B,Ns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: the game's opening theme was sung by\n",
      "Generated text: l t .........................................\n"
     ]
    }
   ],
   "source": [
    "# Initialize decoder with BOS token\n",
    "input_text = \"the game's opening theme was sung by\"\n",
    "inputs = model.tokenizer(input_text, \n",
    "                      padding=True, \n",
    "                      truncation=True, \n",
    "                      return_tensors=\"pt\")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"].transpose(0, 1).to('cuda')  # [Ns,B]\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.model.eval()\n",
    "model.model.to('cuda')\n",
    "\n",
    "# Generate sequence using the model's generate method\n",
    "with torch.no_grad():\n",
    "    # Generate up to 50 new tokens\n",
    "    generated_ids = model.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "    \n",
    "    # Decode the generated sequence\n",
    "    input_text = model.tokenizer.decode(input_ids.transpose(0, 1)[0], skip_special_tokens=True)\n",
    "    decoded_text = model.tokenizer.decode(generated_ids.transpose(0, 1)[0], skip_special_tokens=True)\n",
    "    print(f\"Input text: {input_text}\")\n",
    "    print(f\"Generated text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded text: the game's opening theme was sung by\n"
     ]
    }
   ],
   "source": [
    "decoded = model.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvib_sa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
