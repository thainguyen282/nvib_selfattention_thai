{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip NLTK imports entirely - use alternative approach\n",
    "# We'll import the data loading functions we need without the full data module\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['NLTK_DATA'] = '/home/tqn/.conda/envs/nvib_sa/lib/python3.10/site-packages/nltk_data'\n",
    "\n",
    "import nltk\n",
    "# Force NLTK to initialize properly\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# Cell 2 - Then run your imports\n",
    "from data_modules.ReconstructionDataModule import ReconstructionDataModule, load_prepared_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.nvib_sa_transformer_encoder import (\n",
    "    NVIBTransformerEncoder,\n",
    "    NVIBTransformerEncoderLayer,\n",
    ")\n",
    "from models.seq2seq_lightning import Seq2SeqLightning\n",
    "from models.transformer import *\n",
    "from models.transformer_encoder import (\n",
    "    CustomTransformerEncoder,\n",
    "    CustomTransformerEncoderLayer,\n",
    ")\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEMPORARY: Simple gradient checking - DELETE AFTER USE\n",
    "def check_gradients_simple(model, step_name=\"\"):\n",
    "    \"\"\"Simple gradient vanishing check - temporary function\"\"\"\n",
    "    print(f\"\\nüîç GRADIENT CHECK {step_name}:\")\n",
    "    vanishing_count = 0\n",
    "    exploding_count = 0\n",
    "    total_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad_norm = torch.norm(param.grad).item()\n",
    "            total_params += 1\n",
    "            \n",
    "            if grad_norm < 1e-7:\n",
    "                vanishing_count += 1\n",
    "                if 'decoder' in name:\n",
    "                    print(f\"   ‚ö†Ô∏è  VANISHING in {name}: {grad_norm:.2e}\")\n",
    "            elif grad_norm > 100:\n",
    "                exploding_count += 1\n",
    "                if 'decoder' in name:\n",
    "                    print(f\"   ‚ö†Ô∏è  EXPLODING in {name}: {grad_norm:.2e}\")\n",
    "    \n",
    "    print(f\"   Total params: {total_params}, Vanishing: {vanishing_count}, Exploding: {exploding_count}\")\n",
    "    if vanishing_count > total_params * 0.1:\n",
    "        print(\"   üö® GRADIENT VANISHING DETECTED!\")\n",
    "    return vanishing_count, exploding_count\n",
    "\n",
    "# TEMPORARY: Alpha overflow checking - DELETE AFTER USE\n",
    "def check_alpha_overflow(alpha_dict, step_name=\"\"):\n",
    "    \"\"\"Check for alpha overflow/underflow issues\"\"\"\n",
    "    print(f\"\\nüìä ALPHA CHECK {step_name}:\")\n",
    "    \n",
    "    for key, tensor in alpha_dict.items():\n",
    "        if tensor is not None and torch.is_tensor(tensor):\n",
    "            # Check for NaN/Inf\n",
    "            has_nan = torch.isnan(tensor).any().item()\n",
    "            has_inf = torch.isinf(tensor).any().item()\n",
    "            \n",
    "            # Get statistics\n",
    "            tensor_max = torch.max(tensor).item()\n",
    "            tensor_min = torch.min(tensor).item()\n",
    "            tensor_mean = torch.mean(tensor).item()\n",
    "            \n",
    "            print(f\"   {key}: shape={tensor.shape}\")\n",
    "            print(f\"      min={tensor_min:.2e}, max={tensor_max:.2e}, mean={tensor_mean:.2e}\")\n",
    "            \n",
    "            if has_nan:\n",
    "                print(f\"      üö® NaN detected in {key}!\")\n",
    "                nan_positions = torch.nonzero(torch.isnan(tensor))\n",
    "                print(f\"      NaN positions: {nan_positions[:5].tolist()}...\")  # Show first 5\n",
    "            \n",
    "            if has_inf:\n",
    "                print(f\"      üö® Inf detected in {key}!\")\n",
    "                inf_positions = torch.nonzero(torch.isinf(tensor))\n",
    "                print(f\"      Inf positions: {inf_positions[:5].tolist()}...\")  # Show first 5\n",
    "            \n",
    "            # Check for potential overflow (very large values)\n",
    "            if tensor_max > 700:  # exp(700) is close to overflow\n",
    "                print(f\"      ‚ö†Ô∏è  Very large values in {key} (max={tensor_max:.2e}) - potential exp overflow!\")\n",
    "            \n",
    "            # Check for potential underflow (very small values after exp)\n",
    "            if 'alpha' in key.lower() and tensor_max < 1e-10:\n",
    "                print(f\"      ‚ö†Ô∏è  Very small alpha values (max={tensor_max:.2e}) - potential underflow!\")\n",
    "    \n",
    "    return has_nan, has_inf\n",
    "\n",
    "# Note:\n",
    "# B: Batch size\n",
    "# Ns: Source length\n",
    "# Nt: Target length\n",
    "# Nl: Latent length (typically = Ns)\n",
    "# E: Embedding dimension\n",
    "# H: Model dimension\n",
    "# V: Vocab dimension\n",
    "\n",
    "\n",
    "class NVIBSaTransformer(Transformer):\n",
    "    \"\"\"\n",
    "    A vanilla Transformer Encoder-Decoder in Pytorch\n",
    "\n",
    "    Data format:\n",
    "    SRC: ... [EOS]\n",
    "    TGT: ... [EOS]\n",
    "    Encoder_input(SRC): ... [EOS]\n",
    "    Decoder_input(TGT): [SOS] ...\n",
    "\n",
    "    For an autoencoder x -> x (SRC = TGT)\n",
    "        The loss function requires SRC and logits.\n",
    "    For different models x -> y (Eg: translation SRC != TGT)\n",
    "        The loss function requires TGT and logits.\n",
    "\n",
    "    If we keep this format the attention masks for padding are identical for autoencoder's encoder + decoder .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, **kwargs):\n",
    "        super().__init__(tokenizer=tokenizer, **kwargs)\n",
    "\n",
    "        self.d_model = kwargs[\"d_model\"]\n",
    "        self.nhead = kwargs[\"nhead\"]\n",
    "        self.dim_feedforward = kwargs[\"dim_feedforward\"]\n",
    "        self.dropout = kwargs[\"dropout\"]\n",
    "        self.num_encoder_layers = kwargs[\"num_encoder_layers\"]\n",
    "        self.num_nvib_encoder_layers = kwargs[\"num_nvib_encoder_layers\"]\n",
    "        self.num_decoder_layers = kwargs[\"num_decoder_layers\"]\n",
    "        self.kappa = kwargs[\"kappa\"]\n",
    "        self.delta = kwargs[\"delta\"]\n",
    "\n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = CustomTransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nhead,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=\"relu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "\n",
    "        # NVIB Transformer encoder layer\n",
    "        nvib_transformer_encoder_layer = NVIBTransformerEncoderLayer(\n",
    "            d_model=self.d_model,\n",
    "            nhead=self.nhead,\n",
    "            dim_feedforward=self.dim_feedforward,\n",
    "            dropout=self.dropout,\n",
    "            activation=\"relu\",\n",
    "            kappa=self.kappa,\n",
    "            delta=self.delta,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        encoder_norm = nn.LayerNorm(self.d_model, eps=1e-5)\n",
    "        self.encoder = CustomTransformerEncoder(\n",
    "            encoder_layer, self.num_encoder_layers, encoder_norm\n",
    "        )\n",
    "        self.nvib_transformer_encoder = NVIBTransformerEncoder(\n",
    "            nvib_transformer_encoder_layer, self.num_nvib_encoder_layers, encoder_norm\n",
    "        )\n",
    "\n",
    "        # Transformer decoder\n",
    "        decoder_layer = CustomTransformerDecoderLayer(\n",
    "            self.d_model,\n",
    "            self.nhead,\n",
    "            self.dim_feedforward,\n",
    "            self.dropout,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        decoder_norm = nn.LayerNorm(self.d_model, eps=1e-5)\n",
    "        self.decoder = CustomTransformerDecoder(\n",
    "            decoder_layer, self.num_decoder_layers, decoder_norm\n",
    "        )\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.decoder_start_token_id = tokenizer.cls_token_id\n",
    "        self.args = kwargs\n",
    "        self.embedding = nn.Embedding(tokenizer.vocab_size, self.d_model, padding_idx=0)\n",
    "        self.positional_encoding = PositionalEncoding(self.d_model)\n",
    "        self.output_proj = nn.Linear(self.d_model, tokenizer.vocab_size)\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "        \n",
    "        # TEMPORARY: Add gradient hooks for debugging - DELETE AFTER USE\n",
    "        self._gradient_debug_hooks = []\n",
    "        self._gradient_debug_step = 0\n",
    "        self._gradient_debug_frequency = 100  # Print every 100 steps\n",
    "        self._add_gradient_hooks()\n",
    "\n",
    "    # TEMPORARY: Gradient debugging methods - DELETE AFTER USE\n",
    "    def _add_gradient_hooks(self):\n",
    "        \"\"\"Add hooks to monitor decoder gradients\"\"\"\n",
    "        def make_hook(name):\n",
    "            def hook(grad):\n",
    "                if grad is not None and torch.isnan(grad).any():\n",
    "                    print(f\"üö® NaN gradient in {name}!\")\n",
    "                    print(f\"   Grad shape: {grad.shape}\")\n",
    "                    print(f\"   Grad norm: {torch.norm(grad).item():.2e}\")\n",
    "            return hook\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            if 'decoder' in name and param.requires_grad:\n",
    "                hook = param.register_hook(make_hook(name))\n",
    "                self._gradient_debug_hooks.append(hook)\n",
    "\n",
    "    def enable_gradient_debug(self):\n",
    "        \"\"\"Enable gradient debugging - call this before training\"\"\"\n",
    "        self._gradient_debug_enabled = True\n",
    "        print(\"üîç Gradient debugging ENABLED\")\n",
    "        \n",
    "        # TEMPORARY: Test if debugging is working\n",
    "        print(f\"üîç Debug enabled: {hasattr(self, '_gradient_debug_enabled')}\")\n",
    "        print(f\"üîç Number of gradient hooks: {len(self._gradient_debug_hooks)}\")\n",
    "        print(\"üîç Ready to monitor gradients and alpha values!\")\n",
    "\n",
    "    def print_final_gradient_summary(self):\n",
    "        \"\"\"Print final gradient summary before finishing\"\"\"\n",
    "        if hasattr(self, '_gradient_debug_enabled'):\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üèÅ FINAL GRADIENT SUMMARY (Step {self._gradient_debug_step})\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Force a gradient check regardless of frequency\n",
    "            old_step = self._gradient_debug_step\n",
    "            self._gradient_debug_step = (self._gradient_debug_step // self._gradient_debug_frequency + 1) * self._gradient_debug_frequency\n",
    "            check_gradients_simple(self, f\"FINAL_STEP_{old_step}\")\n",
    "            self._gradient_debug_step = old_step\n",
    "            \n",
    "            print(f\"Total steps monitored: {self._gradient_debug_step}\")\n",
    "            print(f\"Monitoring frequency: every {self._gradient_debug_frequency} steps\")\n",
    "            print(f\"{'='*60}\")\n",
    "\n",
    "    def disable_gradient_debug(self):\n",
    "        \"\"\"Disable gradient debugging and clean up\"\"\"\n",
    "        self.print_final_gradient_summary()  # Print final summary\n",
    "        \n",
    "        if hasattr(self, '_gradient_debug_enabled'):\n",
    "            delattr(self, '_gradient_debug_enabled')\n",
    "        for hook in self._gradient_debug_hooks:\n",
    "            hook.remove()\n",
    "        self._gradient_debug_hooks = []\n",
    "        print(\"üîç Gradient debugging DISABLED\")\n",
    "\n",
    "    def encode(self, src, src_key_padding_mask):\n",
    "        \"\"\"\n",
    "        Encode the input ids to embeddings and then pass to the transformer encoder\n",
    "        :param src: source token ids [Ns, B]\n",
    "        :param src_key_padding_mask: Trues where to mask [B,Ns]\n",
    "        :return: memory: [Ns,B,H]\n",
    "        \"\"\"\n",
    "        # Add position encodings + Embeddings\n",
    "        src = self.positional_encoding(self.drop(self.embedding(src)))  # [Ns,B,H]\n",
    "\n",
    "        # Transformer encoder\n",
    "        memory1, attention1 = self.encoder(\n",
    "            src, src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [Ns,B,H]\n",
    "\n",
    "        # NVIB Transformer encoder\n",
    "        memory2, attention2, klg, kld, latent_dict = self.nvib_transformer_encoder(\n",
    "            memory1[-1], src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [Ns,B,H]\n",
    "        # Concatenate the attention lists\n",
    "        attention = attention1 + attention2\n",
    "        return memory2, attention, klg, kld, latent_dict, memory1\n",
    "\n",
    "\n",
    "    def decode(\n",
    "        self, tgt, z, memory_key_padding_mask, tgt_key_padding_mask, *args, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "\n",
    "        :param tgt: target token ids [Nt,B]\n",
    "        :param z: output from the latent layer [Nl,B,H]\n",
    "        :param memory_key_padding_mask: mask for latent layer [B, Nl] (typically Ns = Nl)\n",
    "        :param tgt_key_padding_mask: target mask [B,Nt]\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return: logits over the vocabulary [Nt,B,V]\n",
    "        \"\"\"\n",
    "\n",
    "        # Add position encodings + Embeddings\n",
    "        tgt = self.positional_encoding(self.drop(self.embedding(tgt)))  # [Nt,B,H]\n",
    "        \n",
    "        # Normalize and use tanh for smooth, differentiable value bounding\n",
    "        tgt = F.layer_norm(tgt, [tgt.size(-1)])\n",
    "        tgt = 100 * torch.tanh(tgt / 100)  # Smooth bound to [-100, 100]\n",
    "        \n",
    "        # Generate target teacher forcing mask\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(0)).to(\n",
    "            tgt.device\n",
    "        )  # [Nt, Nt]\n",
    "        \n",
    "        # Normalize and bound memory input\n",
    "        z = F.layer_norm(z, [z.size(-1)])\n",
    "        z = 100 * torch.tanh(z / 100)  # Smooth bound to [-100, 100]\n",
    "        \n",
    "        output, attention = self.decoder(\n",
    "            tgt=tgt,  # [Nt,B,H]\n",
    "            memory=z,  # [Nt,B,H]\n",
    "            tgt_mask=tgt_mask,  # [Nt,Nt]\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,  # [B,Nt]\n",
    "            memory_key_padding_mask=memory_key_padding_mask,\n",
    "        )  # [B,Nl]\n",
    "        assert not torch.isnan(output).any(), (\n",
    "            f\"NaN values detected in output after decoder layer at {self.__class__.__name__}. \"\n",
    "            f\"NaN indices: {torch.nonzero(torch.isnan(output), as_tuple=True)}. \"\n",
    "            f\"Shape: {output.shape}, Max value: {torch.max(output)}\"\n",
    "            f\"Output: {output}\"\n",
    "        )\n",
    "        # Normalize decoder output\n",
    "        output = F.layer_norm(output, [output.size(-1)])\n",
    "        \n",
    "        # Differentiable gradient norm scaling\n",
    "        norm = torch.norm(output, p=2, dim=-1, keepdim=True)\n",
    "        scale = torch.min(\n",
    "            torch.ones_like(norm),\n",
    "            5 / (norm + 1e-6)  # Target norm of 5\n",
    "        )\n",
    "        output = output * scale\n",
    "        \n",
    "        # Smooth value bounding\n",
    "        output = 100 * torch.tanh(output / 100)\n",
    "        \n",
    "        # Apply output projection\n",
    "        logits = self.output_proj(output)  # [Nt,B,V]\n",
    "        \n",
    "        # Final smooth bounding on logits\n",
    "        logits = 100 * torch.tanh(logits / 100)\n",
    "        \n",
    "        return logits, attention\n",
    "\n",
    "    def generate(self, input_ids, max_new_tokens, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Generate autoregressively without teacher forcing\n",
    "        :param z: output from the latent layer [Nl,B,H]\n",
    "        :param memory_key_padding_mask: mask from the latent layer [B,Nl]\n",
    "        :param max_len: maximum generation length\n",
    "        :param tokenizer: tokenizer\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return: logits [Nt,B,V] and list of predictions\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        src_key_padding_mask = ~(input_ids.bool()).transpose(0, 1)  # [B,Ns]\n",
    "        memory, _, _, _, self_attention_latent, _ = self.encode(\n",
    "            input_ids, src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [Ns,B,H]\n",
    "\n",
    "        # Mask the src_key_padding_mask with the final latent layer's pi for cross attention\n",
    "        src_key_padding_mask = src_key_padding_mask + self_attention_latent[-1][\n",
    "            \"alpha\"\n",
    "        ].squeeze(-1).transpose(0, 1)[:, 1:].le(0.1)\n",
    "\n",
    "        # Soft weighting of vectors\n",
    "        # memory = memory * self_attention_latent[-1][\"pi\"][1:, :, :]\n",
    "\n",
    "        # latent layer\n",
    "        latent_output_dict = self.latent_layer(memory, src_key_padding_mask)\n",
    "        memory_key_padding_mask = latent_output_dict[\"memory_key_padding_mask\"]\n",
    "        z = latent_output_dict[\"z\"]\n",
    "\n",
    "        # Initialise target ids with BOS token\n",
    "        target_ids = (\n",
    "            torch.tensor([[self.decoder_start_token_id]])\n",
    "            .expand(memory_key_padding_mask.shape[0], -1)\n",
    "            .T.to(memory_key_padding_mask.device)\n",
    "        )  # [1, B]\n",
    "        # For each token in length\n",
    "        for token_idx in range(max_new_tokens):\n",
    "            # Decode the target ids regressively\n",
    "            logits, _ = self.decode(\n",
    "                target_ids,\n",
    "                z,\n",
    "                memory_key_padding_mask,\n",
    "                None,\n",
    "                # latent_dict=self_attention_latent[-1]\n",
    "            )  # [token_idx, B, V]\n",
    "            # Select only the final set of logits\n",
    "            prediction = logits[-1, :, :].unsqueeze(0)  # [target_ids1,B,V]\n",
    "            # Get prediction over vocabulary and return index\n",
    "            prediction = prediction.argmax(-1)  # [1,B]\n",
    "            # Concatenate the predictions to form next token_ids\n",
    "            target_ids = torch.cat((target_ids, prediction), dim=0)  # [token_index, B]\n",
    "\n",
    "        # Decode into a sentence\n",
    "        # predictions = [tokenizer.decode(encoded) for encoded in target_ids[1:, :].T]  # list [B]\n",
    "        return target_ids[1:, :]\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        decoder_input_ids,\n",
    "        labels,\n",
    "        attention_mask,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass for all transformer models\n",
    "\n",
    "        :param src: the sequence to the encoder (required). [Ns,B]\n",
    "        :param tgt: the sequence  nce to the decoder (required). [Nt,B]\n",
    "        :param src_mask: the additive mask for the src sequence (optional). [Ns, Ns]\n",
    "        :param tgt_mask: the additive mask for the tgt sequence (optional). [Nt, Nt]\n",
    "        :param memory_mask: the additive mask for the encoder output (optional). [Nt,Ns]\n",
    "        :param src_key_padding_mask: the ByteTensor mask for src keys per batch (optional). [B,Ns]\n",
    "        :param tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional). [B,Nt]\n",
    "        :param memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).[B,Nl]\n",
    "        :return: logits and latent dimension dictionary\n",
    "\n",
    "        Check out here for more info masks on https://stackoverflow.com/questions/62170439/difference-between-src-mask-and-src-key-padding-mask\n",
    "        The memory ones are interesting. I use memory_key_padding_mask to mask the tokens in the latent space.\n",
    "\n",
    "        \"\"\"\n",
    "        # TEMPORARY: Check if forward is being called with debugging - DELETE AFTER USE\n",
    "        if hasattr(self, '_gradient_debug_enabled'):\n",
    "            self._gradient_debug_step += 1\n",
    "            if self._gradient_debug_step % self._gradient_debug_frequency == 0:\n",
    "                print(f\"üîç Step {self._gradient_debug_step}: Forward pass with gradient debugging\")\n",
    "        # Reformat the attention mask\n",
    "        \n",
    "        # Check if there is all True in attention_mask\n",
    "        if attention_mask is not None:\n",
    "            all_zero_sequences = torch.all(attention_mask == 0, dim=-1)\n",
    "            if torch.any(all_zero_sequences):\n",
    "                assert False, f\"Found {torch.sum(all_zero_sequences)} sequences with all 0s in attention_mask\"\n",
    "        src_key_padding_mask = ~(attention_mask.bool())\n",
    "        tgt_key_padding_mask = decoder_input_ids.transpose(0, 1) == self.pad_token_id\n",
    "        assert not torch.isnan(input_ids).any(), (\n",
    "            f\"NaN values detected in input_ids at {self.__class__.__name__}. \"\n",
    "            f\"NaN indices: {torch.nonzero(torch.isnan(input_ids), as_tuple=True)}. \"\n",
    "            f\"Shape: {input_ids.shape}, Max value: {torch.max(input_ids)}\"\n",
    "            f\"Input ids: {input_ids}\"\n",
    "        )\n",
    "        # Encode\n",
    "        (\n",
    "            memory,\n",
    "            encoder_attention,\n",
    "            klg,\n",
    "            kld,\n",
    "            self_attention_latent,\n",
    "            old_memory,\n",
    "        ) = self.encode(\n",
    "            input_ids, src_key_padding_mask=src_key_padding_mask\n",
    "        )  # [Ns,B,H]\n",
    "        assert not torch.isnan(memory).any(), (\n",
    "            f\"NaN values detected in memory after encode at {self.__class__.__name__}. \"\n",
    "            f\"Shape: {memory.shape}, Max value: {torch.max(memory)}\"\n",
    "        )\n",
    "        \n",
    "                 # TEMPORARY: Quick alpha overflow check after encode - DELETE AFTER USE\n",
    "        if hasattr(self, '_gradient_debug_enabled') and self._gradient_debug_step % self._gradient_debug_frequency == 0:\n",
    "            alpha_check = self_attention_latent[-1].get(\"alpha\")\n",
    "            log_alpha_check = self_attention_latent[-1].get(\"log_alpha\") \n",
    "            if alpha_check is not None:\n",
    "                alpha_max = torch.max(alpha_check).item()\n",
    "                alpha_has_nan = torch.isnan(alpha_check).any().item()\n",
    "                alpha_has_inf = torch.isinf(alpha_check).any().item()\n",
    "                if alpha_max > 1e10 or alpha_has_nan or alpha_has_inf:\n",
    "                    print(f\"üö® Step {self._gradient_debug_step}: ALPHA ISSUE after encode: max={alpha_max:.2e}, NaN={alpha_has_nan}, Inf={alpha_has_inf}\")\n",
    "                else:\n",
    "                    print(f\"‚úÖ Step {self._gradient_debug_step}: ALPHA OK after encode: max={alpha_max:.2e}\")\n",
    "            if log_alpha_check is not None:\n",
    "                log_alpha_max = torch.max(log_alpha_check).item()\n",
    "                if log_alpha_max > 700:  # Close to exp overflow\n",
    "                    print(f\"‚ö†Ô∏è  Step {self._gradient_debug_step}: LOG_ALPHA very large: max={log_alpha_max:.2e} - potential exp(log_alpha) overflow!\")\n",
    "        # Mask the src_key_padding_mask with the final latent layer's pi for cross attention\n",
    "        alpha_tensor = self_attention_latent[-1][\"alpha\"]\n",
    "        \n",
    "        # TEMPORARY: Check alpha for overflow - DELETE AFTER USE\n",
    "        if hasattr(self, '_gradient_debug_enabled') and self._gradient_debug_step % self._gradient_debug_frequency == 0:\n",
    "            # Check all latent dict items for potential overflow\n",
    "            # Focus on log_alpha (before exp) and alpha (after exp) to detect overflow\n",
    "            alpha_items_to_check = {\n",
    "                'log_alpha_before_exp': self_attention_latent[-1].get(\"log_alpha\"),\n",
    "                'alpha_after_exp': self_attention_latent[-1].get(\"alpha\"),\n",
    "                'pi': self_attention_latent[-1].get(\"pi\"),\n",
    "                'mu': self_attention_latent[-1].get(\"mu\"),\n",
    "                'logvar': self_attention_latent[-1].get(\"logvar\")\n",
    "            }\n",
    "            check_alpha_overflow(alpha_items_to_check, f\"LATENT_DICT_STEP_{self._gradient_debug_step}\")\n",
    "        \n",
    "        # Check for alpha values that would cause all-masked rows (leading to NaN in attention)\n",
    "        alpha_for_masking = alpha_tensor.squeeze(-1).transpose(0, 1)[:, 1:]  # [B, Ns-1] (excluding prior token)\n",
    "        alpha_mask = alpha_for_masking.le(0.1)  # Boolean mask where alpha <= 0.1\n",
    "        \n",
    "        # Check if any sequence has ALL alpha values <= 0.1 (would cause all-inf attention mask row)\n",
    "        all_masked_sequences = alpha_mask.all(dim=1)  # [B] - True where entire sequence would be masked\n",
    "        \n",
    "        if all_masked_sequences.any():\n",
    "            problematic_batches = torch.nonzero(all_masked_sequences, as_tuple=False).squeeze(-1)\n",
    "            print(f\"üö® CRITICAL: Found {all_masked_sequences.sum().item()} sequences with ALL alpha <= 0.1!\")\n",
    "            print(f\"Problematic batch indices: {problematic_batches.tolist()}\")\n",
    "            print(f\"This will create all-inf attention mask rows leading to NaN after softmax!\")\n",
    "            \n",
    "            # Show details for first few problematic sequences\n",
    "            for i, batch_idx in enumerate(problematic_batches[:3]):\n",
    "                alpha_values = alpha_for_masking[batch_idx]\n",
    "                print(f\"Batch {batch_idx.item()}: alpha values = {alpha_values}\")\n",
    "                print(f\"Batch {batch_idx.item()}: min_alpha = {alpha_values.min().item():.6f}, max_alpha = {alpha_values.max().item():.6f}\")\n",
    "                print(f\"Batch {batch_idx.item()}: src_key_padding_mask before = {src_key_padding_mask[batch_idx]}\")\n",
    "        \n",
    "        if src_key_padding_mask is not None:\n",
    "            all_true_sequences = torch.all(src_key_padding_mask, dim=-1)\n",
    "            if torch.any(all_true_sequences):\n",
    "                assert False, f\"Found {torch.sum(all_true_sequences)} sequences with all True in src_key_padding_mask\"\n",
    "        \n",
    "        # src_key_padding_mask = src_key_padding_mask + alpha_mask\n",
    "        # if alpha_mask is not None:\n",
    "        #     all_true_sequences = torch.all(alpha_mask, dim=-1)\n",
    "        #     if torch.any(all_true_sequences):\n",
    "        #         assert False, f\"Found {torch.sum(all_true_sequences)} sequences with all True in alpha_mask\"\n",
    "        \n",
    "        # if src_key_padding_mask is not None:\n",
    "        #     all_zero_sequences = torch.all(src_key_padding_mask, dim=-1)\n",
    "        #     if torch.any(all_zero_sequences):\n",
    "        #         assert False, f\"Found {torch.sum(all_zero_sequences)} sequences with all 0s in src_key_padding_mask\"\n",
    "        \n",
    "\n",
    "        # Soft weighting of vectors\n",
    "        # memory = memory * self_attention_latent[-1][\"pi\"][1:, :, :]\n",
    "\n",
    "        # latent layer\n",
    "        latent_output_dict = self.latent_layer(memory, src_key_padding_mask)\n",
    "        # Decode\n",
    "        assert not torch.isnan(latent_output_dict[\"z\"]).any(), (\n",
    "            f\"NaN values detected in latent_output_dict['z'] at {self.__class__.__name__}. \"\n",
    "            f\"Shape: {latent_output_dict['z'].shape}, Max value: {torch.max(latent_output_dict['z'])}\"\n",
    "        )\n",
    "\n",
    "        output, decoder_attention = self.decode(\n",
    "            tgt=decoder_input_ids,  # [Nt,B,H]\n",
    "            z=latent_output_dict[\"z\"],  # [Nl,B,H]\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,  # [B,Nt]\n",
    "            memory_key_padding_mask=latent_output_dict[\"memory_key_padding_mask\"],\n",
    "            # latent_dict=self_attention_latent[-1],\n",
    "        )  # [B,Nl]\n",
    "        assert not torch.isnan(output).any(), (\n",
    "            f\"NaN values detected in output decode at {self.__class__.__name__}. \"\n",
    "            f\"Shape: {output.shape}, Max value: {torch.max(output)}\"\n",
    "        )\n",
    "\n",
    "        # TEMPORARY: Add backward hook to check gradients - DELETE AFTER USE\n",
    "        def backward_hook():\n",
    "            if hasattr(self, '_gradient_debug_enabled') and self._gradient_debug_step % self._gradient_debug_frequency == 0:\n",
    "                check_gradients_simple(self, f\"AFTER_DECODE_STEP_{self._gradient_debug_step}\")\n",
    "        \n",
    "        if hasattr(self, '_gradient_debug_enabled') and output.requires_grad:\n",
    "            output.register_hook(lambda grad: backward_hook())\n",
    "        # Check if there is all 0 in attention_mask\n",
    "        return {\n",
    "            \"logits\": output,  # [Nt, B, V]\n",
    "            \"encoder_attentions\": encoder_attention,  # Self attention\n",
    "            \"cross_attentions\": decoder_attention,  # Cross attention\n",
    "            \"kl_gaussian\": klg,\n",
    "            \"kl_dirichlet\": kld,\n",
    "            \"latent_dict_list\": self_attention_latent,\n",
    "            \"old_memory\": old_memory,\n",
    "            \"old_memory_mask\": ~(attention_mask.bool()),\n",
    "        }\n",
    "\n",
    "class NVIBSaTransformerLightning(Seq2SeqLightning):\n",
    "    def __init__(self, args, **kwargs):\n",
    "        super().__init__(args, **kwargs)\n",
    "\n",
    "        # Tokenizer\n",
    "        self.tokenizer = CharizardTokenizer(model_max_length=args.max_length)\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "        # Model\n",
    "        self.model = NVIBSaTransformer(tokenizer=self.tokenizer, **vars(args))\n",
    "        # self.model.enable_gradient_debug()\n",
    "        # print(\"‚úÖ Called enable_gradient_debug() in Lightning model\")\n",
    "        # print(f\"‚úÖ Model has debug attribute: {hasattr(self.model, '_gradient_debug_enabled')}\")\n",
    "        \n",
    "        # TEMPORARY: Register cleanup for crashes - DELETE AFTER USE\n",
    "        import atexit\n",
    "        atexit.register(self.model.print_final_gradient_summary)\n",
    "        # Nvib\n",
    "        self.lambda_klg = args.klg_lambda\n",
    "        self.lambda_kld = args.kld_lambda\n",
    "\n",
    "        # Logging metrics\n",
    "        self.log_bleu = True\n",
    "        self.log_chrf = True\n",
    "        self.plot_encoder_attention = True\n",
    "        self.plot_cross_attention = True\n",
    "        self.model_type = \"NVIBSaTransformer\"\n",
    "        self.is_nvib = True\n",
    "        self.weighted_kl = args.weighted_kl\n",
    "\n",
    "        # Initialization\n",
    "        init_weights(self.model)\n",
    "\n",
    "        self.save_hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # Paths + Naming\n",
    "    experiment_name: str = \"nvib-paper\"\n",
    "    project_name: str = \"nvib_selfattention\"\n",
    "    output_dir: str = \"outputs\"\n",
    "    entity: str = None\n",
    "\n",
    "    # Data\n",
    "    data: str = \"wikitext\"\n",
    "    data_subset: str = \"wikitext-2-raw-v1\"\n",
    "    num_workers: int = 17\n",
    "    max_length: int = 128\n",
    "\n",
    "    # Model\n",
    "    model: str = \"NVIBSaTransformer\"\n",
    "\n",
    "    # Training\n",
    "    seed: int = 42\n",
    "    fast_dev_run: bool = False\n",
    "    fp16: bool = False\n",
    "\n",
    "    # Transformer\n",
    "    d_model: int = 512\n",
    "    d_compress_model: int = 512\n",
    "    nhead: int = 8\n",
    "    num_encoder_layers: int = 3\n",
    "    num_decoder_layers: int = 2\n",
    "    dim_feedforward: int = 512\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    # Token deletion probability\n",
    "    deletion_prob: float = 0\n",
    "    deletion_type: str = \"token\"  # choices: [\"token\", \"word\", \"token_word\", \"None\"]\n",
    "\n",
    "    # NVIB\n",
    "    num_nvib_encoder_layers: int = 1\n",
    "    kappa: float = 1\n",
    "    delta: float = 1\n",
    "    klg_lambda: float = 0.001\n",
    "    kld_lambda: float = 1\n",
    "    kl_annealing_type: str = \"constant\"\n",
    "    weighted_kl: bool = True\n",
    "\n",
    "    # Learning rate + Schedulers\n",
    "    learning_rate: float = 1e-4\n",
    "    lr_scheduler: bool = False\n",
    "    perc_warmup: float = 0\n",
    "    batch_size: int = 1\n",
    "\n",
    "    # PL Trainer\n",
    "    max_time: str = None\n",
    "    max_steps: int = 1000\n",
    "    max_epochs: int = None\n",
    "    accumulate_grad_batches: int = 1\n",
    "    checkpoint_interval: int = 100\n",
    "    validation_interval: int = None\n",
    "\n",
    "    def __init__(self):\n",
    "        # Make sure all attributes are properly initialized\n",
    "        for attr_name in dir(self):\n",
    "            if not attr_name.startswith('__'):\n",
    "                setattr(self, attr_name, getattr(self, attr_name))\n",
    "\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    \"Transformer\": TransformerLightning,\n",
    "    \"NVIBSaTransformer\": NVIBSaTransformerLightning,\n",
    "}[args.model](args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model type: {args.model}\")\n",
    "print(f\"d_model: {args.d_model}\")\n",
    "print(f\"num_nvib_encoder_layers: {args.num_nvib_encoder_layers}\")\n",
    "print(f\"weighted_kl: {args.weighted_kl}\")\n",
    "print(f\"klg_lambda: {args.klg_lambda}\")\n",
    "print(f\"kld_lambda: {args.kld_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"/project/phan/tqn/new_Adapter/nvib_selfattention/outputs/nvib_selfattention/nvib-debug13/\"\n",
    "CHECKPOINT_PATH = get_checkpoint_path(OUTPUT_PATH)\n",
    "BEST_MODEL_PATH = \"/project/phan/tqn/new_Adapter/nvib_selfattention/outputs/nvib_selfattention/nvib-debug13/best_model.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_PATH = \"/project/phan/tqn/new_Adapter/nvib_selfattention/outputs/nvib_selfattention/nvib-debug13/epoch=50-step=8000.ckpt\"\n",
    "print(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, wandb_id = create_or_load_model(OUTPUT_PATH, CHECKPOINT_PATH, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage - load data without ReconstructionDataModule\n",
    "# First, make sure you have run train.py to generate the tokenized data\n",
    "\n",
    "# If you need to use the data loading functionality:\n",
    "# 1. Make sure train.py has been run to generate the preprocessed data\n",
    "# 2. Use the direct loading function instead\n",
    "\n",
    "# Example:\n",
    "# model = your_model  # Your transformer model\n",
    "# tokenizer = model.tokenizer\n",
    "# data_loader = load_prepared_data_direct(\n",
    "#     tokenizer=tokenizer,\n",
    "#     name=\"train\", \n",
    "#     data_name=\"wikitext\",\n",
    "#     model_name=\"Transformer\"\n",
    "# )\n",
    "\n",
    "print(\"Use load_prepared_data_direct() instead of importing ReconstructionDataModule\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_modules.ReconstructionDataModule import ReconstructionDataModule, load_prepared_data\n",
    "dict_args = vars(args)\n",
    "dm = ReconstructionDataModule(model, **dict_args)\n",
    "data_loader = load_prepared_data(\n",
    "    tokenizer=dm.tokenizer,\n",
    "    name=\"train\",\n",
    "    data_name=dm.data,\n",
    "    model_name=dm.model_name,\n",
    ")\n",
    "for batch in data_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_attention, strip_after_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize decoder with BOS token\n",
    "input_text = \"homarus gammarus, known as the european lobster or common lobster, is a species of clawed lobster from the eastern atlantic\"\n",
    "inputs = model.tokenizer(input_text,\n",
    "                      return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "input_ids = inputs[\"input_ids\"].transpose(0, 1).to('cuda')  # [Ns,B]\n",
    "\n",
    "# Set model to evaluation mode\n",
    "# model.model.eval()\n",
    "model.model.to('cuda')\n",
    "model.model.eval()\n",
    "\n",
    "# Generate sequence using the model's generate method\n",
    "# with torch.no_grad():\n",
    "# Generate up to 50 new tokens\n",
    "generated_ids = model.model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "# Decode the generated sequence\n",
    "# input_text = model.tokenizer.decode(input_ids.transpose(0, 1), skip_special_tokens=True)\n",
    "decoded_text = model.tokenizer.batch_decode(generated_ids.transpose(0, 1))\n",
    "decoded_text = strip_after_token(decoded_text, model.tokenizer.sep_token)\n",
    "print(f\"Input text: {input_text}\")\n",
    "print(f\"Generated text: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = model.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True)\n",
    "print(f\"Decoded text: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the hypothesis: Compare training vs eval mode\n",
    "input_text = \"homarus gammarus, known as the european lobster or common lobster, is a species of clawed lobster from the eastern atlantic\"\n",
    "inputs = model.tokenizer(input_text, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].transpose(0, 1).to('cuda')  # [Ns,B]\n",
    "print(input_ids.transpose(0, 1))\n",
    "print(\"=== TRAINING MODE (like validation) ===\")\n",
    "model.model.train()  # Set to training mode\n",
    "with torch.no_grad():\n",
    "    generated_ids_train = model.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "    decoded_text_train = model.tokenizer.batch_decode(generated_ids_train.transpose(0, 1), skip_special_tokens=True)\n",
    "    print(f\"Training mode output: {decoded_text_train}\")\n",
    "\n",
    "print(\"\\n=== EVAL MODE (deterministic) ===\")\n",
    "model.model.eval()  # Set to eval mode\n",
    "with torch.no_grad():\n",
    "    generated_ids_eval = model.model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=5\n",
    "    )\n",
    "    decoded_text_eval = model.tokenizer.batch_decode(generated_ids_eval.transpose(0, 1), skip_special_tokens=True)\n",
    "    print(f\"Eval mode output: {decoded_text_eval}\")\n",
    "\n",
    "print(\"\\n=== COMPARISON ===\")\n",
    "print(f\"Same output? {decoded_text_train == decoded_text_eval}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvib_sa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
